{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "def get_records():\n",
    "    \"\"\" Get paths for data in data/mit/ directory \"\"\"\n",
    "    #Download if doesn't exist\n",
    "    \n",
    "    # There are 3 files for each record\n",
    "    # *.atr is one of them\n",
    "    paths = glob.glob('mitbih/*.atr') # returns an array of path names that matches the arguement\n",
    "    #paths = [os.path.join(os.getcwd(),path) for path in paths]\n",
    "    # Get rid of the extension\n",
    "    paths = [path[:-4] for path in paths]\n",
    "    paths.sort()\n",
    "\n",
    "    return paths\n",
    "\n",
    "records = get_records()\n",
    "print ('There are {} record files'.format(len(records)))\n",
    "print (records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_annotations(annotation, type):\n",
    "    \"\"\" Get rid of non-beat markers \"\"\"\n",
    "    \"\"\"'N' for normal beats. Similarly we can give the input 'L' for left bundle branch block beats. 'R' for right bundle branch block\n",
    "        beats. 'A' for Atrial premature contraction. 'V' for ventricular premature contraction. '/' for paced beat. 'E' for Ventricular\n",
    "        escape beat.\"\"\"\n",
    "    \n",
    "    good = [type] # ['N']   \n",
    "    ids = np.in1d(annotation.symbol, good)\n",
    "\n",
    "    # We want to know only the positions\n",
    "    beats = annotation.sample[ids]\n",
    "\n",
    "    return beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "\n",
    "e = records[4]\n",
    "print(e[-3:])\n",
    "\n",
    "signals, fields = wfdb.rdsamp(e, channels = [0])\n",
    "print('signals=\\n{}\\nfields=\\n{}'.format(signals,fields))\n",
    "plt.plot(signals[0:720],linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A  --  Atrial premature beat\n",
    "f  --  Fusion of paced and normal beat\n",
    "L  --  Left bundle branch block beat\n",
    "N or .  --  Normal beat\n",
    "Q  --  Unclassifiable beat\n",
    "R  --  Right bundle branch block beat\n",
    "V  --  Premature ventricular contraction\n",
    "!  --  Ventricular flutter wave\n",
    "/  --  Paced beat\n",
    "|  --  Isolated QRS-like artifact\n",
    "~  --  \n",
    "+  --\n",
    "\"\"\"\n",
    "\n",
    "ann = wfdb.rdann(e, 'atr')\n",
    "\n",
    "print('annotator symbol=\\n{}\\nannotator sample=\\n{}'.format(ann.symbol,ann.sample))\n",
    "print('annotator symbol length=\\n{}\\nannotator sample length=\\n{}'.format(len(ann.symbol),len(ann.sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_beats = beat_annotations(ann, '/')\n",
    "print('imp_beats=\\n{}'.format(imp_beats))\n",
    "beats = (ann.sample)\n",
    "print('beats=\\n{}'.format(beats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "result = []\n",
    "x_axis = []\n",
    "for i in imp_beats:\n",
    "    beats = np.array(beats)\n",
    "    index_i = np.where(beats == i) # find the indexes (location tuples) of all imp_beats(desired annotated beats) inside the array of all beats  \n",
    "    # print(\"i:\", index_i )\n",
    "    j = index_i[0][0]\n",
    "    # print(\"j:\", j)\n",
    "    if(j!=0 and j!=(len(beats)-1)):\n",
    "            x = beats[j-1]\n",
    "            y = beats[j+1]\n",
    "            # print('x={}, y={}'.format(x,y))\n",
    "            # diff1 = abs(x - beats[j])//2  # // --> floor division e.g. 15//2 = floor(15/2)=7\n",
    "            # diff2 = abs(y - beats[j])//2\n",
    "            # print('diff1={}, diff2={}'.format(diff1,diff2))\n",
    "            sig_start = beats[j-1]+20\n",
    "            sig_end = beats[j+1]-20\n",
    "            #print('diff1={}, diff2={}'.format(sig_start,sig_end))\n",
    "            data =signals[sig_start:sig_end, 0]\n",
    "            #print('data: ',len(data))\n",
    "            result.append(data)\n",
    "            plot_y = [j * 1 for j in range(sig_start, sig_end)]\n",
    "            x_axis.append(plot_y)\n",
    "            #print('plot_y: ',len(plot_y))\n",
    "            #print('data={}'.format(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=300, frameon=False, figsize=(1.0,0.5))\n",
    "plt.plot(result[5], linewidth=0.5)\n",
    "plt.xticks([]), plt.yticks([])\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "filename = 'fig_example.png'\n",
    "fig.savefig(filename)\n",
    "#plt.close()\n",
    "im_gray = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "im_h, im_w = im_gray.shape # as grayscale image so third param (channel) = 1\n",
    "# im_gray = im_gray[20:im_h-20, 30:im_w-30] #crop image\n",
    "im_gray = cv2.copyMakeBorder(im_gray,75,75,0,0, cv2.BORDER_REPLICATE) # as the image shape (from plt.savefig) is 300px*150px\n",
    "# cv2.imshow('', im_gray)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "print(im_gray.shape)\n",
    "im_gray = cv2.resize(im_gray, (128,128), interpolation=cv2.INTER_LANCZOS4)\n",
    "cv2.imwrite('fig_example_resize.png', im_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image',im_gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noOfCols = 5\n",
    "noOfRows = math.ceil(len(result)/noOfCols)\n",
    "fig,axes = plt.subplots(noOfRows,noOfCols, figsize=(15,30))  # figsize=(width (in inches),height (in inches)) of frame \n",
    "k=0\n",
    "for i in range (0,noOfRows): \n",
    "    for j in range (0,noOfCols):\n",
    "        axes[i,j].plot(result[k])\n",
    "        k+=1\n",
    "        if k==len(result):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "\n",
    "#def segmentation(records, type,record_index,output_dir=''):\n",
    "def segmentation(records, type,output_dir=''):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = []\n",
    "    #Normal = []\n",
    "    kernel = np.ones((4, 4), np.uint8)\n",
    "    count = 1\n",
    "    \n",
    "    \n",
    "#     max_values = []\n",
    "#     min_values = []\n",
    "#     mean_values = []\n",
    "#     for e in tqdm(records):\n",
    "#         signals, fields = wfdb.rdsamp(e, channels=[0])\n",
    "#         mean_values.append(np.mean(signals))\n",
    "\n",
    "#     mean_v = np.mean(np.array(mean_values))\n",
    "#     std_v = 0\n",
    "#     count = 0\n",
    "#     for e in tqdm(records):\n",
    "#         signals, fields = wfdb.rdsamp(e, channels=[0])\n",
    "#         count += len(signals)\n",
    "#         for s in signals:\n",
    "#             std_v += (s[0] - mean_v)**2\n",
    "\n",
    "#     std_v = np.sqrt(std_v/count)\n",
    "\n",
    "    mean_v = -0.33859\n",
    "    std_v = 0.472368\n",
    "    floor = mean_v - 3*std_v\n",
    "    ceil = mean_v + 3*std_v\n",
    "    \n",
    "    # tqdm adds progressbar\n",
    "    for e in tqdm(records):\n",
    "        signals, fields = wfdb.rdsamp(e, channels = [0])\n",
    "        # print(signals)\n",
    "        # fig= plt.figure(figsize=(12,3))\n",
    "        # axes= fig.add_axes([0.1,0.1,0.8,0.8]) # dimensions [left, bottom, width, height] of the new axes\n",
    "        # axes.plot(signals)\n",
    "        # plt.show()\n",
    "        # print(fields)\n",
    "        ann = wfdb.rdann(e, 'atr')\n",
    "        # print('annotator symbol:', ann.symbol)\n",
    "        # print('annotator sample:', ann.sample)\n",
    "        #good = ['N']\n",
    "        #ids = np.in1d(ann.symbol, good)\n",
    "        imp_beats = beat_annotations(ann, type) #ann.sample[ids]\n",
    "        # print(imp_beats)\n",
    "        beats = (ann.sample)\n",
    "        for i in tqdm(imp_beats):\n",
    "            #beats = list(beats)\n",
    "            #j = beats.index(i)\n",
    "            beats = np.array(beats)\n",
    "            index_i = np.where(beats == i)\n",
    "            j = index_i[0][0] # as numpy.where returns tuples we only need the first index of item that match\n",
    "            if(j!=0 and j!=(len(beats)-1)):\n",
    "                x = beats[j-1]\n",
    "                y = beats[j+1]\n",
    "                diff1 = abs(x - beats[j])//2  # // --> floor division e.g. 15//2 = floor(15/2)=7\n",
    "                diff2 = abs(y - beats[j])//2\n",
    "                data =signals[beats[j] - diff1: beats[j] + diff2, 0]\n",
    "                #Normal.append(data)\n",
    "                results.append(data)\n",
    "                \n",
    "                plt.axis([0, 192, floor, ceil])\n",
    "                plt.plot(data, linewidth=0.5)\n",
    "                plt.xticks([]), plt.yticks([])\n",
    "                for spine in plt.gca().spines.values():\n",
    "                    spine.set_visible(False)\n",
    "\n",
    "                filename = output_dir + 'fig_{}_{}'.format(records.index(e),count) + '.png'\n",
    "                #filename = output_dir + 'fig_{}_{}'.format(record_index,count) + '.png'\n",
    "                plt.savefig(filename)\n",
    "                plt.close()\n",
    "                im_gray = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "                im_gray = cv2.erode(im_gray, kernel, iterations=1)\n",
    "                im_gray = cv2.resize(im_gray, (128, 128), interpolation=cv2.INTER_LANCZOS4)\n",
    "                cv2.imwrite(filename, im_gray)\n",
    "                print('img writtten {}'.format(filename))\n",
    "                count += 1\n",
    "        print('img completed {}'.format(e))\n",
    "    #return Normal\n",
    "    return results\n",
    "    \n",
    "\n",
    "# commenting/uncommenting a block of code: select the block of code then press  'ctrl'+'/'\n",
    "\n",
    "# testing with one signal file that the segmentation function will work fine\n",
    "# import math\n",
    "\n",
    "# test_records = [records[2]]\n",
    "# signalArr = segmentation(test_records)\n",
    "# print(len(signalArr))\n",
    "# noOfCols = 5\n",
    "# noOfRows = math.ceil(len(signalArr)/noOfCols)\n",
    "# # plt.rcParams['figure.figsize'] = [5,5] # [width (in inches), height (in inches)]\n",
    "# fig,axes = plt.subplots(noOfRows,noOfCols, figsize=(15,30))  # figsize=(width (in inches),height (in inches)) of frame \n",
    "# k=0\n",
    "# for i in range (0,noOfRows): \n",
    "#     for j in range (0,noOfCols):\n",
    "#         axes[i,j].plot(signalArr[k])\n",
    "#         k+=1\n",
    "#         if k==len(signalArr):\n",
    "#             break\n",
    "# #print(signalArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating database by segmentation of ecg beats into image\n",
    "\n",
    "labels = ['A', 'L', 'N', '/', 'V', 'R', 'E', '!']\n",
    "output_dirs = ['APC/', 'LBBB/', 'NOR/', 'PAB/', 'PVC/', 'RBBB/', 'VEB/', 'VFE/']\n",
    "#one_record=[records[47]]\n",
    "for type, output_dir in zip(labels, output_dirs):\n",
    "    sgs = segmentation(records, type, output_dir='./MIT-BIH_AD/'+output_dir)\n",
    "    #sgs = segmentation(one_record, type, record_index=47, output_dir='./MIT-BIH_AD/'+output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the length of different directory inside dataset\n",
    "Database_DIR = 'MIT-BIH_AD_BASE/'\n",
    "image_dirs = ['NOR/', 'LBBB/', 'RBBB/', 'APC/', 'PVC/', 'VEB/','PAB/', 'VFE/']\n",
    "no_of_files_in_dir=[]\n",
    "for image_dir in image_dirs:\n",
    "    path, dirs, files = next(os.walk(os.path.join(Database_DIR,image_dir)))\n",
    "    no_of_files_in_dir.append(len(files)) \n",
    "\n",
    "print('Number of images in each directory={} and total number of images={}'.format(no_of_files_in_dir, sum(no_of_files_in_dir)))\n",
    "plt.figure(figsize=(10,10))\n",
    "my_circle=plt.Circle((0,0), 0.7, color='white')\n",
    "plt.pie(no_of_files_in_dir, labels=['NOR', 'LBBB', 'RBBB', 'APC', 'PVC', 'VEB','PAB', 'VFE'], colors=['red','green','blue','skyblue','orange', 'yellow','magenta', 'cyan'],autopct='%1.1f%%')\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()\n",
    "#p.savefig('data_distribution.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation paper function\n",
    "def cropping(image, filename):\n",
    "    \n",
    "    #Left Top Crop\n",
    "    crop = image[:96, :96]\n",
    "    crop = cv2.resize(crop, (128, 128))\n",
    "    cv2.imwrite(filename[:-4] + 'leftTop' + '.png', crop)\n",
    "    \n",
    "    #Center Top Crop\n",
    "    crop = image[:96, 16:112]\n",
    "    crop = cv2.resize(crop, (128, 128))\n",
    "    cv2.imwrite(filename[:-4] + 'centerTop' + '.png', crop)\n",
    "    \n",
    "    #Right Top Crop\n",
    "    crop = image[:96, 32:]\n",
    "    crop = cv2.resize(crop, (128, 128))\n",
    "    cv2.imwrite(filename[:-4] + 'rightTop' + '.png', crop)\n",
    "    \n",
    "    #Left Center Crop\n",
    "    crop = image[16:112, :96]\n",
    "    crop = cv2.resize(crop, (128, 128))\n",
    "    cv2.imwrite(filename[:-4] + 'leftCenter' + '.png', crop)\n",
    "    \n",
    "    #Center Center Crop\n",
    "    crop = image[16:112, 16:112]\n",
    "    crop = cv2.resize(crop, (128, 128))\n",
    "    cv2.imwrite(filename[:-4] + 'centerCenter' + '.png', crop)\n",
    "    \n",
    "    #Right Center Crop\n",
    "    crop = image[16:112, 32:]\n",
    "    crop = cv2.resize(crop, (128, 128))\n",
    "    cv2.imwrite(filename[:-4] + 'rightCenter' + '.png', crop)\n",
    "    \n",
    "    #Left Bottom Crop\n",
    "    crop = image[32:, :96]\n",
    "    crop = cv2.resize(crop, (128, 128))\n",
    "    cv2.imwrite(filename[:-4] + 'leftBottom' + '.png', crop)\n",
    "    \n",
    "    #Center Bottom Crop\n",
    "    crop = image[32:, 16:112]\n",
    "    crop = cv2.resize(crop, (128, 128))\n",
    "    cv2.imwrite(filename[:-4] + 'centerBottom' + '.png', crop)\n",
    "    \n",
    "    #Right Bottom Crop\n",
    "    crop = image[32:, 32:]\n",
    "    crop = cv2.resize(crop, (128, 128))\n",
    "    cv2.imwrite(filename[:-4] + 'rightBottom' + '.png', crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "augment_dirs = ['LBBB/', 'RBBB/', 'APC/', 'PVC/', 'VEB/','PAB/', 'VFE/']\n",
    "# print(os.path.join(Database_DIR,image_dir,'fig_28_1.png'))\n",
    "# img = cv2.imread(os.path.join(Database_DIR,image_dir,'fig_28_1.png'))\n",
    "# cv2.imshow('cropped',img)\n",
    "# cv2.waitKey(0)\n",
    "for image_dir in augment_dirs:\n",
    "    path, dirs, files = next(os.walk(os.path.join(Database_DIR,image_dir)))\n",
    "    for file in tqdm(files):\n",
    "        imagefilepath = os.path.join(Database_DIR,image_dir,file)\n",
    "        image = cv2.imread(imagefilepath)\n",
    "        cropping(image, imagefilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the length of different directory inside dataset\n",
    "no_of_files_in_dir_new=[]\n",
    "for image_dir in image_dirs:\n",
    "    path, dirs, files = next(os.walk(os.path.join(Database_DIR,image_dir)))\n",
    "    no_of_files_in_dir_new.append(len(files)) \n",
    "\n",
    "print('Number of images in each directory={} and total number of images={}'.format(no_of_files_in_dir_new, sum(no_of_files_in_dir_new)))\n",
    "plt.figure(figsize=(10,10))\n",
    "my_circle=plt.Circle((0,0), 0.7, color='white')\n",
    "plt.pie(no_of_files_in_dir_new, labels=['NOR', 'LBBB', 'RBBB', 'APC', 'PVC', 'VEB','PAB', 'VFE'], colors=['red','green','blue','skyblue','orange', 'yellow','magenta', 'cyan'],autopct='%1.1f%%')\n",
    "p1=plt.gcf()\n",
    "p1.gca().add_artist(my_circle)\n",
    "plt.show()\n",
    "#p1.savefig('data_distribution_after_augmentation.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Activation, Conv2D, Dense, Dropout, Flatten, MaxPool2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print('Number of GPU available', len(public_devices))\n",
    "\n",
    "if len(public_devices) > 0:\n",
    "    for gpu in public_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)  # preventing tensorflow to allocate all gpu memory at start of declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only a pc with rtx2070 super gpu need this block\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data images into train, test, validation directory\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# os.chdir('Dataset')\n",
    "\n",
    "image_dirs = ['NOR', 'LBBB', 'RBBB', 'APC', 'PVC', 'VEB','PAB', 'VFE']\n",
    "\n",
    "if os.path.isdir('MIT-BIH_AD/train/APC') is False:\n",
    "    for i in image_dirs:\n",
    "        current_path = 'MIT-BIH_AD/'+i\n",
    "        path_train = 'MIT-BIH_AD/train/'+i\n",
    "        path_valid = 'MIT-BIH_AD/valid/'+i\n",
    "        path_test = 'MIT-BIH_AD/test/'+i\n",
    "        os.makedirs(path_train)\n",
    "        os.makedirs(path_valid)\n",
    "        os.makedirs(path_test)\n",
    "        path, dirs, files = next(os.walk(current_path))\n",
    "        no_of_files = len(files)\n",
    "        no_of_valid_dir_files = round(no_of_files*0.1)\n",
    "        no_of_test_dir_files = round(no_of_files*0.2)\n",
    "        no_of_train_dir_files = no_of_files - (no_of_test_dir_files + no_of_valid_dir_files)\n",
    "        print(no_of_files)\n",
    "        for j in random.sample(glob.glob(current_path+'/fig*'),no_of_train_dir_files):\n",
    "            shutil.move(j,path_train)\n",
    "        for j in random.sample(glob.glob(current_path+'/fig*'),no_of_valid_dir_files):\n",
    "            shutil.move(j,path_valid)\n",
    "        for j in random.sample(glob.glob(current_path+'/fig*'),no_of_test_dir_files):\n",
    "            shutil.move(j,path_test)\n",
    "        \n",
    "        \n",
    "# moving 70%,10%,20% data from dataset/with_mask (no of data=1915) directory to train,valid,test /with_mask directory    \n",
    "# moving 70%,10%,20% data from dataset/without_mask (no of data=1918) directory to train,valid,test /without_mask directory  \n",
    "\n",
    "# os.chdir('../')\n",
    "\n",
    "# no need to run after first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the length of different directory inside dataset\n",
    "Database_DIR = 'MIT-BIH_AD/valid/'\n",
    "image_dirs = ['NOR/', 'LBBB/', 'RBBB/', 'APC/', 'PVC/', 'VEB/','PAB/', 'VFE/']\n",
    "no_of_files_in_dir=[]\n",
    "for image_dir in image_dirs:\n",
    "    path, dirs, files = next(os.walk(os.path.join(Database_DIR,image_dir)))\n",
    "    no_of_files_in_dir.append(len(files)) \n",
    "\n",
    "print('Number of images in each directory={} and total number of images={}'.format(no_of_files_in_dir, sum(no_of_files_in_dir)))\n",
    "plt.figure(figsize=(10,10))\n",
    "my_circle=plt.Circle((0,0), 0.7, color='white')\n",
    "plt.pie(no_of_files_in_dir, labels=['NOR', 'LBBB', 'RBBB', 'APC', 'PVC', 'VEB','PAB', 'VFE'], colors=['red','green','blue','skyblue','orange', 'yellow','magenta', 'cyan'],autopct='%1.1f%%')\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()\n",
    "#p.savefig('data_distribution.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images_arr, batchSize, subplot_dim=[1,10]):\n",
    "    fig, axes = plt.subplots(subplot_dim[0], subplot_dim[1], figsize=(20,20))\n",
    "    axes = axes.flatten()  # flaten converts an array to a 1D vector\n",
    "    for img, ax in zip(images_arr,axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, crop_tuple):\n",
    "    \"\"\"crop_tuple consist of 4 params (y_start, y_end, x_start, x_end)\"\"\"\n",
    "    assert len(image.shape) == 3 # confirms the image shape to be of length 3 (image_h, image_w, no_of_channels)\n",
    "    assert image.shape[2] == 3 # confirms image data format is channels_last  (image_h, image_w, no_of_channels) and RGB channel\n",
    "    \n",
    "    img_height, img_width = image.shape[0], image.shape[1]\n",
    "    y_start, y_end, x_start, x_end = crop_tuple\n",
    "    \n",
    "    if not x_start:\n",
    "        x_start = 0\n",
    "    if not y_start:\n",
    "        y_start = 0\n",
    "    if not x_end:\n",
    "        x_end = img_width-1\n",
    "    if not y_end:\n",
    "        y_end = img_height-1\n",
    "    \n",
    "    cropped_img = image[y_start:y_end, x_start:x_end]  # cropping image according to given coordinates\n",
    "    cropped_img = cv2.resize(cropped_img, (img_height, img_width)) # resizing image to its original size\n",
    "    \n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_generator(batches, crop_tuples_arr):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        \n",
    "        assert batch_x.shape[3] == 3 # confirms batch data format is channels_last  (batch_size,image_h, image_w, no_of_channels) and RGB channel\n",
    "        \n",
    "        # plot the batch images after cropped\n",
    "        plotImages(batch_x, batch_x.shape[0], [4,8])\n",
    "        print(batch_y)\n",
    "        print('separator\\n')\n",
    "        \n",
    "        batch_crops = np.zeros(batch_x.shape) # tensor of 0s with shape (batch_size,image_h, image_w, no_of_channels)\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = crop_image(batch_x[i], crop_tuple)\n",
    "        \n",
    "        # plot the batch images after cropped\n",
    "        plotImages(batch_crops, batch_x.shape[0], [4,8])\n",
    "        print(batch_y)\n",
    "        \n",
    "        yield (batch_crops, batch_y) # yield --> instead of return, denotes the function is a generator, keeps local states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"MIT-BIH_DATABASE/train\"\n",
    "test_path = \"MIT-BIH_DATABASE/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size*epoch = number_of_iteration*batch_size\n",
    "\n",
    "batchSize = 4 #32\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1./255, validation_split=0.1)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_uncropped = train_gen.flow_from_directory(directory=train_path, target_size=(128,128), classes=['APC', 'LBBB', 'NOR', 'PAB', 'PVC', 'RBBB', 'VEB', 'VFE'], batch_size=batchSize, seed=32)\n",
    "test_batches = test_gen.flow_from_directory(directory=test_path, target_size=(128,128), classes=['APC', 'LBBB', 'NOR', 'PAB', 'PVC', 'RBBB', 'VEB', 'VFE'], batch_size=batchSize, seed=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to set the crop_touple in a way in which,  x_end-x_start = y_end-y_start  to avoid distortion in output image\n",
    "#crop_coordinates_arr = [actual image, Left Top Crop, Center Top Crop, Right Top Crop, Left Center Crop, Center Center Crop, Right Center Crop, Left Bottom Crop, Center Bottom Crop, Right Bottom Crop]\n",
    "\n",
    "crop_coordinates_arr = [(0,128,0,128), (0,96,0,96), (0,96,16,112), (0,96,32,128), (16,112,0,96), (16,112,16,112), (16,112,32,128), (32,128,0,96), (32,128,16,112), (32,128,32,128)]\n",
    "\n",
    "# train_batches = crop_generator(train_batches_uncropped, crop_coordinates_arr)\n",
    "\n",
    "train_batches = crop_generator(train_batches_uncropped, (0,96,0,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print the generator \"crop_generator\" output\n",
    "next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, crop_tuple):\n",
    "    \"\"\"crop_tuple consist of 4 params (x_start, y_start, x_end, y_end)\"\"\"\n",
    "    assert len(image.shape) == 3 # confirms the image shape to be of length 3 (image_h, image_w, no_of_channels)\n",
    "    assert image.shape[2] == 3 # confirms image data format is channels_last  (image_h, image_w, no_of_channels) and RGB channel\n",
    "    \n",
    "    img_height, img_width = image.shape[0], image.shape[1]\n",
    "    x_start, y_start, x_end, y_end = crop_tuple\n",
    "    \n",
    "    if not x_start:\n",
    "        x_start = 0\n",
    "    if not y_start:\n",
    "        y_start = 0\n",
    "    if not x_end:\n",
    "        x_end = img_width-1\n",
    "    if not y_end:\n",
    "        y_end = img_height-1\n",
    "    \n",
    "    cropped_img = image[y_start:y_end, x_start:x_end]  # cropping image according to given coordinates\n",
    "    cropped_img = cv2.resize(cropped_img, (img_height, img_width)) # resizing image to its original size\n",
    "#     cv2.imshow('',cropped_img)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_generator(batches, crop_tuple):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        \n",
    "        assert batch_x.shape[3] == 3 # confirms batch data format is channels_last  (batch_size,image_h, image_w, no_of_channels) and RGB channel\n",
    "        \n",
    "        batch_crops = np.zeros(batch_x.shape) # tensor of 0s with shape (batch_size,image_h, image_w, no_of_channels)\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = crop_image(batch_x[i], crop_tuple)\n",
    "        yield (batch_crops, batch_y) # yield --> instead of return, denotes the function is a generator, keeps local states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'Augmentation_test_folder/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 1\n",
    "train_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train_gen.flow_from_directory(directory=train_path, target_size=(128,128), classes=['c1', 'c2'], batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_crops = crop_generator(train_batches, (0,0,96,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_crops, batch_y = next(train_batches)\n",
    "print(batch_y)\n",
    "plotImages(batch_crops, batchSize, [2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(train_path+'/fig_100_18.png')\n",
    "x = img_to_array(img) # numpy array with shape (128,128,3)\n",
    "print(x.shape)\n",
    "x = x.reshape((1,)+x.shape) # numpy array with shape (1,128,128,3)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1,0,0,0,0,0,0,0]\n",
    "y = np.array(labels)\n",
    "print(y.shape)\n",
    "y = y.reshape((1,)+y.shape) # numpy array with shape (1,8)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train_gen.flow(x,\n",
    "                              y,\n",
    "                              batch_size=batchSize,\n",
    "                              seed=5) \n",
    "#                               save_to_dir=train_path,\n",
    "#                               save_prefix='aug',\n",
    "#                               save_format='png')\n",
    "\n",
    "train_batches_crops = crop_generator(train_batches, (0,0,128,100))\n",
    "\n",
    "i = 0\n",
    "for batch in train_batches_crops:\n",
    "    i += 1\n",
    "    if i >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"MIT-BIH_AD/train\"\n",
    "valid_path = 'MIT-BIH_AD/valid'\n",
    "test_path = 'MIT-BIH_AD/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size*epoch = number_of_iteration*batch_size\n",
    "\n",
    "batchSize = 32\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "valid_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train_gen.flow_from_directory(directory=train_path, target_size=(128,128), classes=['APC', 'LBBB', 'NOR', 'PAB', 'PVC', 'RBBB', 'VEB', 'VFE'], batch_size=batchSize, seed=32)\n",
    "valid_batches = valid_gen.flow_from_directory(directory=valid_path, target_size=(128,128), classes=['APC', 'LBBB', 'NOR', 'PAB', 'PVC', 'RBBB', 'VEB', 'VFE'], batch_size=batchSize, seed=32)\n",
    "test_batches = test_gen.flow_from_directory(directory=test_path, target_size=(128,128), classes=['APC', 'LBBB', 'NOR', 'PAB', 'PVC', 'RBBB', 'VEB', 'VFE'], batch_size=batchSize, seed=32) #shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking (train) images and labels of only one batch (10 images,10 labels) and plot them\n",
    "\n",
    "imgs, labels = next(train_batches)\n",
    "plotImages(imgs, batchSize, [4,8])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "def proposed_model(input_h, input_w, nb_classes):\n",
    "    InputShape = (input_h, input_w, 3)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(filters=64, kernel_size=(3,3), activation='elu', padding='same', input_shape=InputShape, kernel_initializer='glorot_uniform'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=64, kernel_size=(3,3), activation='elu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D(pool_size=(2, 2), strides= 2),\n",
    "        Conv2D(filters=128, kernel_size=(3,3), activation='elu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=128, kernel_size=(3,3), activation='elu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D(pool_size=(2, 2), strides= 2),\n",
    "        Conv2D(filters=256, kernel_size=(3,3), activation='elu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(filters=256, kernel_size=(3,3), activation='elu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D(pool_size=(2, 2), strides= 2),\n",
    "        Flatten(),\n",
    "        Dense(units=2048, activation='elu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(rate=0.5),\n",
    "        Dense(units=nb_classes, activation='softmax'),\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = proposed_model(128, 128, 8)\n",
    "print(model.summary())\n",
    "# dot_img_file = '/tmp/model_1.png'\n",
    "# tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "model.compile(optimizer=Adam(learning_rate= lr), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "Epoch = 1\n",
    "Verbose = 1\n",
    "\n",
    "model.fit(x=train_batches,validation_data=valid_batches,epochs=Epoch,verbose=Verbose)\n",
    "\n",
    "# save model (architecture, optimizer, weights, ...all)\n",
    "if os.path.isdir('models') is False:\n",
    "    os.makedirs('models')\n",
    "if os.path.isfile('models/ecg_arrgythmia_detection_model.h5') is False:\n",
    "    model.save('models/ecg_arrgythmia_detection_model.h5')\n",
    "    print('model saved successfully.')\n",
    "    \n",
    "# 100/3510 [.............................] ETA: 50:00 - loss: 1.18 - accuracy: 0.68\n",
    "# running bacth no / total number of training batches [.....progressbar(shown when verbose=1)......] ETA(estimated time of (result) arrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "prev_saved_model = load_model('models/ecg_arrgythmia_detection_model.h5')\n",
    "#prev_saved_model = load_model('models/cnn.h5')\n",
    "\n",
    "print(prev_saved_model.summary())\n",
    "# print(prev_saved_model.get_weights())\n",
    "# print(prev_saved_model.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "Epoch = 10\n",
    "Verbose = 1\n",
    "\n",
    "prev_saved_model.fit(x=train_batches,validation_data=valid_batches,epochs=Epoch,verbose=Verbose)\n",
    "prev_saved_model.save('models/ecg_arrgythmia_detection_model.h5')\n",
    "print('model saved successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "\n",
    "predictions = prev_saved_model.predict(x = test_batches, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_batches.classes\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix plot function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix_custom(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\" prints and plots confusion matrix. \n",
    "        normalization can be applied by setting `normalize=True` \"\"\"\n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print('Normalized confusion matrix')\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization ')\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max() / 2\n",
    "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:0.2f}\".format(cm[i, j]), horizontalalignment=\"center\", color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('confusion.jpg', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix output\n",
    "cm = confusion_matrix(y_true = test_labels, y_pred = rounded_predictions)\n",
    "cm_plot_lables = ['NOR', 'LBBB', 'RBBB', 'APC', 'PVC', 'VEB','PAB', 'VFE']\n",
    "# non normalized confusion matrix\n",
    "#plot_confusion_matrix_custom(cm = cm, classes = cm_plot_lables)\n",
    "\n",
    "# normalized confusion matrix\n",
    "plot_confusion_matrix_custom(cm = cm, classes = cm_plot_lables, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report_result = classification_report(test_labels, rounded_predictions, target_names=cm_plot_lables)\n",
    "print(classification_report_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
